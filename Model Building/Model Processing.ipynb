{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Model processing notebook\n",
    "%pwd\n",
    "#u'/media/alex/ssd/programs/Galvanize/00_project/Email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from os import listdir\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Welcome to the Model Processing Notebook for EmailRank\n",
    "\n",
    "##This notebook takes the processed data and trains the models used for classification. Models are then pickled for use in the flask app.\n",
    "\n",
    "### Steps here:\n",
    "\n",
    "1. Import data from pickle file\n",
    "1. Run text processing function to get feature matrix\n",
    "1. Run cross-validation on a classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13373 entries, 0 to 13372\n",
      "Data columns (total 3 columns):\n",
      "FROM    13373 non-null object\n",
      "TO      13373 non-null object\n",
      "TEXT    13373 non-null object\n",
      "dtypes: object(3)"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "\n",
    "## Read in data and perform some calculations for better understanding. \n",
    "## Data input is from the Email processing workbook\n",
    "\n",
    "filepath = '../flask_app/data/data pickles/full_data.pkl'\n",
    "data = pd.read_pickle(filepath)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TO</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CEO</th>\n",
       "      <td>  159</td>\n",
       "      <td>  159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Director</th>\n",
       "      <td>  177</td>\n",
       "      <td>  177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee</th>\n",
       "      <td> 1814</td>\n",
       "      <td> 1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In House Lawyer</th>\n",
       "      <td>   18</td>\n",
       "      <td>   18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manager</th>\n",
       "      <td>  502</td>\n",
       "      <td>  502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Managing Director</th>\n",
       "      <td>  209</td>\n",
       "      <td>  209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>President</th>\n",
       "      <td>  467</td>\n",
       "      <td>  467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trader</th>\n",
       "      <td>  296</td>\n",
       "      <td>  296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vice President</th>\n",
       "      <td> 1317</td>\n",
       "      <td> 1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\N</th>\n",
       "      <td>   41</td>\n",
       "      <td>   41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TO  TEXT\n",
       "TO                           \n",
       "CEO                 159   159\n",
       "Director            177   177\n",
       "Employee           1814  1814\n",
       "In House Lawyer      18    18\n",
       "Manager             502   502\n",
       "Managing Director   209   209\n",
       "President           467   467\n",
       "Trader              296   296\n",
       "Vice President     1317  1317\n",
       "\\N                   41    41\n",
       "\n",
       "[10 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['TO','TEXT']].groupby('TO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FROM</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FROM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CEO</th>\n",
       "      <td>  113</td>\n",
       "      <td>  113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Director</th>\n",
       "      <td>   77</td>\n",
       "      <td>   77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employee</th>\n",
       "      <td> 2305</td>\n",
       "      <td> 2305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In House Lawyer</th>\n",
       "      <td>    4</td>\n",
       "      <td>    4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manager</th>\n",
       "      <td>  650</td>\n",
       "      <td>  650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Managing Director</th>\n",
       "      <td>  126</td>\n",
       "      <td>  126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>President</th>\n",
       "      <td>  161</td>\n",
       "      <td>  161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trader</th>\n",
       "      <td>  177</td>\n",
       "      <td>  177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vice President</th>\n",
       "      <td> 1366</td>\n",
       "      <td> 1366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\N</th>\n",
       "      <td>   21</td>\n",
       "      <td>   21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FROM  TEXT\n",
       "FROM                         \n",
       "CEO                 113   113\n",
       "Director             77    77\n",
       "Employee           2305  2305\n",
       "In House Lawyer       4     4\n",
       "Manager             650   650\n",
       "Managing Director   126   126\n",
       "President           161   161\n",
       "Trader              177   177\n",
       "Vice President     1366  1366\n",
       "\\N                   21    21\n",
       "\n",
       "[10 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['FROM','TEXT']].groupby('FROM').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Step two is used to create the text processor pipeline. This is a custom class that includes the text processors\n",
    "## pos tagger, and tfidf tokenizer trained on the input data. This class is then pickled for use in the flask app\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "## Define UPENN tagset parts of speech (used for column headers / feature importances)\n",
    "PARTSOFSPEECH = ['WRB', 'VBZ', 'DT', 'NN', '.', 'NNP', ',', 'PRP', 'VBD', 'TO',\n",
    "       'NNS', 'IN', 'VBG', 'CC', 'RP', 'RB', 'MD', 'VB', 'VBP', 'JJ', ':',\n",
    "       'CD', 'JJR', 'WDT', 'PRP$', 'VBN', 'POS', 'WP', 'JJS', '$',\n",
    "       '-NONE-', 'NNPS', 'EX']\n",
    "\n",
    "def getPOS(x):\n",
    "    ## Function used to get parts of speech features\n",
    "    ## Input: tokenized email list\n",
    "    ## Output: Counts of parts of speech\n",
    "    ## This exists as part of the multi-processing for the pos tagger.\n",
    "\n",
    "    parts = [i[1] for i in pos_tag(x)]\n",
    "\n",
    "    output = []\n",
    "    for part in PARTSOFSPEECH:\n",
    "        output.append(np.sum(np.array(parts) == part))\n",
    "    return output / (np.linalg.norm(output, ord=2)+1)\n",
    "\n",
    "\n",
    "\n",
    "class Processor(object):\n",
    "    ## Custom processor with fit, transform, and fit_transform methods\n",
    "    ## use of the verbose flag will provide add'l details on train\n",
    "    ## \n",
    "\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.myVect = None\n",
    "    \n",
    "    def fit_transform(self, X, flag=True, verbose=True):\n",
    "        ## Training command for the text processor. Used to fit the tokenizer and pos taggers\n",
    "        ## Flag is used to identify when to fit the tokenizer and should only be called on a \"fit\" version \n",
    "        ## (e.g. not transform)\n",
    "        \n",
    "        if verbose:\n",
    "            print 'Welcome to the fit_transform for the data'\n",
    "        \n",
    "        X = X.reset_index(drop=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print 'Base tfidf start:'\n",
    "        ## TFIDF from sklearn\n",
    "        if flag:\n",
    "            self.myVect = TfidfVectorizer(ngram_range=(2,2))\n",
    "            self.myVect.fit(X)\n",
    "            if verbose:\n",
    "                print 'training successful'\n",
    "            \n",
    "        output = pd.DataFrame(self.myVect.transform(X).toarray())\n",
    "        if verbose:\n",
    "            print 'transform successful, output = ', output.shape\n",
    "\n",
    "        output.columns = self.myVect.vocabulary_\n",
    "        ## Testing purposes\n",
    "        output = pd.DataFrame([1]*X.shape[0])\n",
    "        \n",
    "        ## tokenize words\n",
    "        if verbose:\n",
    "            print 'Tokenizing text'\n",
    "            \n",
    "        tokens = X.map(word_tokenize)\n",
    "        \n",
    "        if verbose:\n",
    "            print 'Tokenizing successful:', tokens.shape\n",
    "        ## Count of words as feature\n",
    "        output['word_count'] = tokens.map(len)\n",
    "        \n",
    "        \n",
    "        ## Part of speech as feature (see getPOS for more detail)\n",
    "        if verbose:\n",
    "            print 'part of speech tagging'\n",
    "        p = Pool(7)\n",
    "        pos = pd.DataFrame(p.map(getPOS, tokens), columns=PARTSOFSPEECH)\n",
    "        if verbose:\n",
    "            print 'Tagging successful:', pos.shape\n",
    "        output = pd.concat([output, pos], axis=1)\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        self.fit_transform(*args)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = self.fit_transform(X, flag=False)\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "training successful\n",
      "transform successful, output =  (1600, 87653)\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "transform successful, output =  (400, 87653)\n",
      "Accuracy score:  0.845\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "training successful\n",
      "transform successful, output =  (1600, 88883)\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "transform successful, output =  (400, 88883)\n",
      "Accuracy score:  0.8525\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "training successful\n",
      "transform successful, output =  (1600, 85458)\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "transform successful, output =  (400, 85458)\n",
      "Accuracy score:  0.8375\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "training successful\n",
      "transform successful, output =  (1600, 90763)\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "transform successful, output =  (400, 90763)\n",
      "Accuracy score:  0.86\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "training successful\n",
      "transform successful, output =  (1600, 88499)\n",
      "Welcome to the fit_transform for the data\n",
      "Base tfidf start:\n",
      "transform successful, output =  (400, 88499)\n",
      "Accuracy score:  0.8375\n",
      "Overall Model accuracy:  0.8465\n",
      "FROM Accuracy score:  0.997\n",
      "TO Accuracy score:  0.9935\n"
     ]
    }
   ],
   "source": [
    "## Step 3 is the model building and testing code. Below we perform cross validation on a model using the data\n",
    "## The code is often commented to perform cross validation OR build the full model for app use based on system memory concerns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "myFold = KFold(len(myData.FROM), n_folds=5, shuffle=True)\n",
    "\n",
    "accuracyScores = []\n",
    "\n",
    "for trainIndex, testIndex in myFold:\n",
    "    myModel = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    myProcessor = Processor()\n",
    "    \n",
    "    trainX = myProcessor.fit_transform(data.TEXT[trainIndex], verbose=True)\n",
    "    trainY = myData.FROM[trainIndex]\n",
    "    \n",
    "    testX = myProcessor.transform(myData.TEXT[testIndex])\n",
    "    testY = myData.FROM[testIndex]\n",
    "    \n",
    "    myModel.fit(trainX, trainY)\n",
    "    predicts = myModel.predict(testX)\n",
    "    \n",
    "    accuracy = accuracy_score(testY, predicts)\n",
    "    accuracyScores.append(accuracy)\n",
    "    print 'Accuracy score: ', accuracy\n",
    "    \n",
    "print 'Overall Model accuracy: ', np.mean(accuracyScores)\n",
    "\n",
    "# Full Rollout code. Commented out due to RAM concerns.\n",
    "\n",
    "#Train Full Model\n",
    "# modelTO = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "# modelFROM = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "# myProcessor = Processor()\n",
    "\n",
    "# trainX = myProcessor.fit_transform(data.TEXT)\n",
    "# trainFROM = myData.FROM\n",
    "# trainTO = myData.TO\n",
    "\n",
    "# testX = myProcessor.transform(myData.TEXT)\n",
    "# testFROM = myData.FROM\n",
    "# testTO = myData.TO\n",
    "\n",
    "# modelFROM.fit(trainX, trainFROM)\n",
    "# modelTO.fit(trainX, trainTO)\n",
    "# predictFROM = modelFROM.predict(testX)\n",
    "# predictTO = modelTO.predict(testX)\n",
    "\n",
    "# accuracyFROM = accuracy_score(testFROM, predictFROM)\n",
    "# accuracyTO = accuracy_score(testTO, predictTO)\n",
    "\n",
    "print 'FROM Accuracy score: ', accuracyFROM\n",
    "print 'TO Accuracy score: ', accuracyTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor pickled!\n",
      "modelTO pickled!\n",
      "modelFROM pickled!\n"
     ]
    }
   ],
   "source": [
    "## Save the model as a pickle for use in Flask\n",
    "\n",
    "with open('FlaskApp/modelPickles/processor.pkl', 'w+') as f:\n",
    "    pickle.dump(myProcessor, f)\n",
    "    print 'Processor pickled!'\n",
    "    \n",
    "with open('FlaskApp/modelPickles/modelTO.pkl', 'w+') as f:\n",
    "    pickle.dump(modelTO, f)\n",
    "    print 'modelTO pickled!'\n",
    "    \n",
    "with open('FlaskApp/modelPickles/modelFROM.pkl', 'w+') as f:\n",
    "    pickle.dump(modelFROM, f)\n",
    "    print 'modelFROM pickled!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP 0.0542446494463 0.647077093554\n",
      "word_count 0.0496102060904 223.0\n",
      "IN 0.0467859161138 0.284082626438\n",
      ". 0.0461399429888 0.0946942088128\n",
      ": 0.0455195419086 0.17360604949\n",
      "NN 0.0447952955396 0.17360604949\n",
      "JJ 0.0436148027561 0.0946942088128\n",
      ", 0.0427035397306 0.205170785761\n",
      "CD 0.0402882115454 0.189388417626\n",
      "PRP 0.0377129817318 0.315647362709\n",
      "DT 0.0365736597669 0.0789118406773\n",
      "NNS 0.035260368393 0.0631294725418\n",
      "VBP 0.0352590417021 0.126258945084\n",
      "TO 0.034795733633 0.126258945084\n",
      "VB 0.0338656716652 0.299864994574\n",
      "RB 0.0328418316732 0.220953153896\n",
      "VBN 0.0323097905149 0.0473471044064\n",
      "VBZ 0.0318777878576 0.0157823681355\n",
      "CC 0.0317019166773 0.0315647362709\n",
      "VBD 0.0295482066587 0.0315647362709\n",
      "VBG 0.0292178693327 0.0157823681355\n",
      "PRP$ 0.0282796805014 0.0789118406773\n",
      "MD 0.0266524208602 0.142041313219\n",
      "POS 0.017779259351 0.0157823681355\n",
      "WDT 0.0154848495593 0.0\n",
      "RP 0.0141433322613 0.0157823681355\n",
      "WP 0.0132793542986 0.0315647362709\n",
      "WRB 0.0122963168377 0.0\n",
      "NNPS 0.0119460448397 0.0\n",
      "JJR 0.0115949532953 0.0\n",
      "-NONE- 0.00953294706989 0.0\n",
      "$ 0.00865279045923 0.0\n",
      "JJS 0.00864210121148 0.0\n",
      "EX 0.00704898372893 0.0\n",
      "-NONE- 0.0 0.0\n",
      "0               1.000000\n",
      "word_count    223.000000\n",
      "WRB             0.000000\n",
      "VBZ             0.015782\n",
      "DT              0.078912\n",
      "NN              0.173606\n",
      ".               0.094694\n",
      "NNP             0.647077\n",
      ",               0.205171\n",
      "PRP             0.315647\n",
      "VBD             0.031565\n",
      "TO              0.126259\n",
      "NNS             0.063129\n",
      "IN              0.284083\n",
      "VBG             0.015782\n",
      "CC              0.031565\n",
      "RP              0.015782\n",
      "RB              0.220953\n",
      "MD              0.142041\n",
      "VB              0.299865\n",
      "VBP             0.126259\n",
      "JJ              0.094694\n",
      ":               0.173606\n",
      "CD              0.189388\n",
      "JJR             0.000000\n",
      "WDT             0.000000\n",
      "PRP$            0.078912\n",
      "VBN             0.047347\n",
      "POS             0.015782\n",
      "WP              0.031565\n",
      "JJS             0.000000\n",
      "$               0.000000\n",
      "-NONE-          0.000000\n",
      "NNPS            0.000000\n",
      "EX              0.000000\n",
      "Name: 51, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Feature importances code\n",
    "\n",
    "#print modelFROM.feature_importances_\n",
    "myProcessor.myVect.vocabulary_\n",
    "\n",
    "FEATURES = ['-NONE-', 'word_count', 'WRB', 'VBZ', 'DT', 'NN', '.', 'NNP', ',', 'PRP', 'VBD', 'TO',\n",
    "       'NNS', 'IN', 'VBG', 'CC', 'RP', 'RB', 'MD', 'VB', 'VBP', 'JJ', ':',\n",
    "       'CD', 'JJR', 'WDT', 'PRP$', 'VBN', 'POS', 'WP', 'JJS', '$',\n",
    "       '-NONE-', 'NNPS', 'EX']\n",
    "\n",
    "top_words = np.array(FEATURES)[np.argsort(modelFROM.feature_importances_)][::-1]\n",
    "for feat, imp in zip(top_words, sorted(modelFROM.feature_importances_)[::-1]):\n",
    "    print feat, imp, trainX.ix[51][feat]\n",
    "# print '#### EOF ################' \n",
    "# top_words = np.array(FEATURES)[np.argsort(modelTO.feature_importances_)][::-1]\n",
    "# for feat, imp in zip(top_words, sorted(modelTO.feature_importances_)[::-1]):\n",
    "#     print feat, imp\n",
    "print trainX.ix[51]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
